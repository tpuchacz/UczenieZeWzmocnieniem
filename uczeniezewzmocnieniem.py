# -*- coding: utf-8 -*-
"""UczenieZeWzmocnieniem

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_goZZlgGqrnbsUlSi0PjM6_y25RNfsDo

#Uczenie ze wzmocnieniem

##FrozenLake

W FrozenLake agent (uczestnik) porusza się po zamarzniętym jeziorze, które jest reprezentowane jako siatka (macierz) komórek. Każda komórka może być jednym z następujących typów:

*   S (Start): Punkt startowy agenta.
*   F (Frozen): Zamarznięte komórki, po których agent może się poruszać.
*   H (Hole): Dziury, których agent musi unikać. Jeśli agent wejdzie na dziurę, epizod kończy się niepowodzeniem.
*   G (Goal): Cel, do którego agent dąży. Dotarcie do celu kończy epizod
sukcesem.

**Cel gry**

Celem agenta jest dotarcie z punktu startowego (S) do celu (G), poruszając się po zamarzniętych komórkach (F) i unikając dziur (H).

**Akcje**

Agent może wykonywać cztery akcje

1.   Lewo
2.   Dół
3.   Prawo
4.   Góra

**Nagrody**

Agent otrzymuje nagrodę:

*   1 punkt za dotarcie do celu (G).
*   0 punktów w każdym innym przypadku, w tym za wejście do dziury (H).

<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*oUW1prwRzihD4ebeS7iu7w.gif" width="300">

**Uczenie ze wzmocnieniem (Reinforcement Learning)**
W kontekście RL, agent uczy się optymalnej polityki (zbioru zasad) poruszania się po środowisku, aby maksymalizować swoją całkowitą nagrodę. Uczenie ze wzmocnieniem polega na interakcji agenta ze środowiskiem poprzez cykliczny proces:


1. Obserwacja: Agent obserwuje aktualny stan środowiska.
2. Akcja: Agent wybiera akcję na podstawie obecnej polityki.
3. Nagroda: Agent otrzymuje nagrodę na podstawie wybranej akcji.
4. Nowy stan: Agent przechodzi do nowego stanu na podstawie wybranej akcji.

**Algorytm Q-learning**
Jednym z popularnych algorytmów RL jest Q-learning. Jest to metoda bez modelu (model-free), co oznacza, że agent nie ma wcześniejszej wiedzy o dynamice środowiska i uczy się wyłącznie na podstawie interakcji. W Q-learningu agent utrzymuje tablicę Q, która przechowuje wartości Q dla każdej pary stan-akcja. Wartość Q jest oczekiwaną skumulowaną nagrodą, jaką agent otrzyma, zaczynając od danego stanu i wykonując daną akcję, a następnie postępując zgodnie z optymalną polityką.

**Etapy działania Q-learningu**
1. Inicjalizacja tablicy Q: Na początku wartości Q są zerowe.
2. Wybór akcji: Agent wybiera akcję na podstawie polityki epsilon-greedy (z pewnym prawdopodobieństwem wybiera losową akcję, aby eksplorować środowisko, a z pozostałym prawdopodobieństwem wybiera najlepszą znaną akcję).
3. Aktualizacja Q: Po wykonaniu akcji agent otrzymuje nagrodę i przechodzi do nowego stanu. Następnie aktualizuje wartość Q dla poprzedniego stanu i akcji na podstawie wzoru:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
$$

Gdzie:

*   s to aktualny stan,
*   a to wykonana akcja,
*   r to otrzymana nagroda,
*   s′ to nowy stan,
*   α to współczynnik uczenia się,
*   γ to współczynnik dyskontowania.
"""

import warnings
warnings.filterwarnings("ignore")
import gym
import numpy as np
import random
from IPython.display import clear_output
import matplotlib.pyplot as plt

# Inicjalizacja środowiska
env = gym.make('FrozenLake-v1', is_slippery=True, render_mode='human', new_step_api=True)

# Parametry Q-learningu
alpha = 0.1  # Współczynnik uczenia się
gamma = 0.99  # Współczynnik dyskontowania
epsilon = 1.0  # Początkowa wartość epsilon dla epsilon-greedy
epsilon_decay = 0.995  # Współczynnik zmniejszania epsilon
min_epsilon = 0.01  # Minimalna wartość epsilon
num_episodes = 5000  # Liczba epizodów treningowych

# Inicjalizacja tablicy Q
Q = np.zeros((env.observation_space.n, env.action_space.n))

# Funkcja wyboru akcji
def choose_action(state, epsilon):
    if random.uniform(0, 1) < epsilon:
        return env.action_space.sample()  # Wybór losowej akcji
    else:
        return np.argmax(Q[state])  # Wybór akcji wg największej wartości Q

# Zmienna do zbierania sum nagród z każdego epizodu
rewards = []

# Główna pętla treningowa
for episode in range(num_episodes):
    state = env.reset()
    done = False
    total_reward = 0
    while not done:
        action = choose_action(state, epsilon)
        #next_state, reward, done, _ = env.step(action)
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        # Aktualizacja wartości Q
        old_value = Q[state, action]
        next_max = np.max(Q[next_state])
        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
        Q[state, action] = new_value

        state = next_state
        total_reward += reward

    rewards.append(total_reward)

    # Zmniejszenie epsilon
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    # Wyświetlanie postępów co 100 epizodów
    if (episode + 1) % 100 == 0:
        clear_output(wait=True)
        print(f'Episode: {episode + 1}, Epsilon: {epsilon:.4f}')


print('Trening zakończony!')

# Testowanie wytrenowanego agenta
state = env.reset()
done = False
while not done:
    action = np.argmax(Q[state])
    #state, reward, done, _ = env.step(action)
    state, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    #env.render()

print('Test zakończony!')

# Funkcja do obliczania średniej kroczącej
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Wyświetlanie wykresu sumy nagród z każdego epizodu
plt.plot(rewards, label='Nagrody')
plt.plot(moving_average(rewards, 50), label='Średnia krocząca')
plt.xlabel('Epizod')
plt.ylabel('Suma nagród')
plt.title('Proces nauki agenta')
plt.legend()
plt.show()

from IPython.display import clear_output
import time

qtable = np.zeros((16, 4))
nb_states = env.observation_space.n  # = 16
nb_actions = env.action_space.n      # = 4
qtable = np.zeros((nb_states, nb_actions))

state = env.reset()
done = False
sequence = []

while not done:
    if np.max(qtable[state]) > 0:
      action = np.argmax(qtable[state])

    else:
      action = env.action_space.sample()

    sequence.append(action)

    #new_state, reward, done, info = env.step(action)
    new_state, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated

    state = new_state

    clear_output(wait=True)
    #env.render()
    time.sleep(1)

print(f"Sequence = {sequence}")